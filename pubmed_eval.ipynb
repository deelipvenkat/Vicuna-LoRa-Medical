{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaConfig, LlamaTokenizer,LlamaForCausalLM, LlamaTokenizer,TextDataset,DataCollatorForLanguageModeling\n",
    "from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig ,LoraConfig,TaskType\n",
    "from transformers import TrainingArguments, Trainer,GenerationConfig,LineByLineTextDataset\n",
    "from datasets import Dataset , load_dataset\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "import textwrap\n",
    "#import bitsandbytes\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/vicuna-weights-7B\"\n",
    "peft_path='/home/medical-llama/saved_models-news'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/medical-llama/saved_models-news/adapter_config.json\", 'r') as json_file:\n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(peft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = LlamaForCausalLM.from_pretrained(path)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        peft_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these properly to avoid errors.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ids = [torch.tensor([835]).to('cuda'),\n",
    "                          torch.tensor([2277, 29937]).to('cuda')]  # '###' can be encoded in two different ways.\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " \n",
    "### Instruction:\n",
    "[INSTRUCTION]\n",
    " \n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(instruction) :\n",
    "    return PROMPT_TEMPLATE.replace(\"[INSTRUCTION]\", instruction)\n",
    "\n",
    "\n",
    "def generate_response(prompt, model: PeftModel) :\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to('cuda')\n",
    " \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "        beam_size=1,\n",
    "        max_length=1,\n",
    "        min_length=1,\n",
    "        max_time=3,\n",
    "        \n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=5,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "    \n",
    "\n",
    "def format_response(response) :\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "\n",
    "def ask_vicuna(prompt, model=model):\n",
    "    prompt = create_prompt(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    b=format_response(response)\n",
    "    a=b.split('###')\n",
    "    return a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_evaluator(Question, Context):\n",
    "    prompt_template=f\"\"\"Task Description:\n",
    "\n",
    "    In this task, you'll be provided with a research question along with a context from a PubMed abstract. Your goal is to answer the research question based on the information given in the context. The answers will be strictly restricted to 'yes', 'no', or 'maybe'.\n",
    "\n",
    "    'Yes' should be used when the information provided in the context clearly supports the posed question.\n",
    "    'No' should be used when the information provided in the context clearly contradicts the posed question.\n",
    "    'Maybe' should be used when the information provided in the context is ambiguous, not enough, or has conflicting data in regard to the posed question.\n",
    "    The question will be related to a medical or biomedical topic.\n",
    "\n",
    "    Question:\n",
    "    {Question}\n",
    "\n",
    "    Context:\n",
    "    {Context}\n",
    "\n",
    "    Remember, your response should be strictly 'yes', 'no', or 'maybe' based on the given context.\n",
    "\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_vicuna(pubmed_evaluator())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
