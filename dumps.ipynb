{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset = load_dataset(\"text\", data_files=\"medical_corpus.txt\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "dataset = TextDataset(tokenized_text, tokenizer,block_size=128)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model & peft weights from huggingface hub\n",
    "\n",
    "peft_model_id = \"smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_counts = Counter(list(ans_dict.values()))\n",
    "\n",
    "for element, count in element_counts.items():\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,f1_macro=[],[]\n",
    "for i in range(5):\n",
    "    ans_dict=model_pubmed_evaluator(model=model,tokenizer=tokenizer,config=pubmed_config,id_=i)\n",
    "    with open(\"/home/medical-llama/pubmed-pred/base.json\", \"w\") as json_file:\n",
    "        json.dump(ans_dict, json_file)\n",
    "    accuracy,f1=pub_scores(\"/home/medical-llama/pubmed-pred/base.json\")\n",
    "    acc.append(accuracy)\n",
    "    f1_macro.append(f1)\n",
    "\n",
    "print(acc)\n",
    "print(f1_macro)\n",
    "\n",
    "print(\"Accuracy: \",np.mean(acc))\n",
    "print(\"F1 Macro: \",np.mean(f1_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
